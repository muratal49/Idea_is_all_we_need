{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7e53c3",
   "metadata": {},
   "source": [
    "# üì¶ 0. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Your API key\n",
    "apikey = 'bgJyXuHdGkrBKt4VsCvR0LeiwE8x39WZ'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1ea2",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def safe_get(text):\n",
    "    return text.strip() if isinstance(text, str) and text.strip() else None\n",
    "\n",
    "def get_abstract_or_intro(paper):\n",
    "    abstract = safe_get(paper.get(\"abstract\"))\n",
    "    if abstract:\n",
    "        return abstract\n",
    "\n",
    "    full_text = paper.get(\"fullText\", \"\")\n",
    "    full_text = clean_text(full_text)\n",
    "\n",
    "    intro_match = re.search(\n",
    "        r'(?:^|\\n)(?:\\d?\\s*INTRODUCTION|BACKGROUND)(?:[:\\.\\n\\s]+)(.*?)(?=\\n[A-Z ]{3,}|[\\n]{2,})',\n",
    "        full_text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if intro_match:\n",
    "        intro = intro_match.group(1).strip()\n",
    "        if len(intro) > 100:\n",
    "            return \"Introduction: \" + intro\n",
    "\n",
    "    return None\n",
    "\n",
    "def query_api(search_url, query, offset=0, limit=20):\n",
    "    headers = {\"Authorization\": \"Bearer \" + apikey}\n",
    "    url = f\"{search_url}?q={query}&limit={limit}&offset={offset}\"\n",
    "\n",
    "    for _ in range(3):  # Retry logic\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                print(f\"Query: {query} | Offset: {offset} | Results: {len(data.get('results', []))}\")\n",
    "                return data, response.elapsed.total_seconds()\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è JSON decode error at offset {offset} for query '{query}'\")\n",
    "                return {\"results\": []}, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error {response.status_code}, retrying...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è Failed after retries for query='{query}', offset={offset}\")\n",
    "    return {\"results\": []}, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9301b",
   "metadata": {},
   "source": [
    "# üßê 2. Section Extraction Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sections(full_text):\n",
    "    if not full_text:\n",
    "        return {\n",
    "            \"conclusions\": \"Full text not available\",\n",
    "            \"limitations\": \"Full text not available\",\n",
    "            \"future_work\": \"Full text not available\"\n",
    "        }\n",
    "\n",
    "    full_text = clean_text(full_text)\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n|\\r\\n\\s*\\r\\n', full_text) if p.strip()]\n",
    "    sections = []\n",
    "    current_section = {\"heading\": \"\", \"content\": \"\"}\n",
    "\n",
    "    for p in paragraphs:\n",
    "        if len(p) < 100 and (p.isupper() or re.match(r'^\\d+[\\.\\s]+\\w+|^[IVX]+[\\.\\s]+\\w+', p)):\n",
    "            if current_section[\"content\"]:\n",
    "                sections.append(current_section)\n",
    "            current_section = {\"heading\": p, \"content\": \"\"}\n",
    "        else:\n",
    "            current_section[\"content\"] += (\" \" + p) if current_section[\"content\"] else p\n",
    "    if current_section[\"content\"]:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    patterns = {\n",
    "        \"limitations\": [\n",
    "            r'\\b(?:limitation|shortcoming|drawback|weakness|constraint)s?\\b',\n",
    "            r'\\bcurrent\\s+(?:limitation|constraint|shortcoming)s?\\b',\n",
    "            r'\\blimiting\\s+factor[s]?\\b',\n",
    "            r'\\bsources?\\s+of\\s+error\\b'\n",
    "        ],\n",
    "        \"future_work\": [\n",
    "            r'\\bfuture\\s+(?:work|research|direction|study|investigation)\\b',\n",
    "            r'\\bfurther\\s+(?:work|research|study|development)\\b',\n",
    "            r'\\bopen\\s+(?:question|issue|challenge|problem|area)s?\\b',\n",
    "            r'\\bwe\\s+plan\\s+to\\b'\n",
    "        ],\n",
    "        \"conclusions\": [\n",
    "            r'\\bconclusion[s]?\\b',\n",
    "            r'\\bconcluding\\s+remarks\\b',\n",
    "            r'\\bin\\s+conclusion\\b',\n",
    "            r'\\bthis\\s+study\\s+(?:shows|demonstrates|confirms|indicates)\\b'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    output = {\"limitations\": \"\", \"future_work\": \"\", \"conclusions\": \"\"}\n",
    "\n",
    "    for section in sections:\n",
    "        heading_lower = section[\"heading\"].lower()\n",
    "        for key, regex_list in patterns.items():\n",
    "            if any(re.search(p, heading_lower) for p in regex_list):\n",
    "                output[key] += f\"Section: {section['heading']}\\n{section['content']}\\n\\n\"\n",
    "\n",
    "    for key, regex_list in patterns.items():\n",
    "        if not output[key]:\n",
    "            matched_sentences = []\n",
    "            for section in sections:\n",
    "                sentences = sent_tokenize(section[\"content\"])\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    if any(re.search(p, sentence.lower()) for p in regex_list):\n",
    "                        context = sentences[max(0, i-1):min(len(sentences), i+2)]\n",
    "                        matched_sentences.append(\" \".join(context))\n",
    "            if matched_sentences:\n",
    "                output[key] = \"Auto-extracted mentions:\\n\" + \"\\n\".join(matched_sentences)\n",
    "\n",
    "    for key in output:\n",
    "        if not output[key]:\n",
    "            output[key] = f\"No {key.replace('_', ' ')} content found\"\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690bfea",
   "metadata": {},
   "source": [
    "# üöÄ 3. Main Loop for Querying and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    search_url = \"https://api.core.ac.uk/v3/search/works\"\n",
    "    topic_queries = [\n",
    "        \"artificial intelligence\", \"machine learning\", \"deep learning\", \"data science\", \"AI applications\",\n",
    "        \"natural language processing\", \"language models\", \"NLP\", \"text mining\", \"information extraction\",\n",
    "        \"computer vision\", \"image recognition\", \"object detection\", \"vision transformers\",\n",
    "        \"biomedical informatics\", \"health informatics\", \"clinical AI\", \"medical imaging\", \"EHR\", \"genomics\",\n",
    "        \"AI ethics\", \"explainable AI\", \"fairness in machine learning\", \"AI in education\", \"social computing\",\n",
    "        \"support vector machines\", \"random forests\", \"decision trees\", \"unsupervised learning\", \"feature selection\",\n",
    "        \"AI systems\", \"distributed learning\", \"edge AI\", \"federated learning\", \"hardware-aware ML\"\n",
    "    ]\n",
    "\n",
    "    max_papers = 2000\n",
    "    limit = 20\n",
    "    all_papers = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    with open(\"core_fulltext_dataset_filtered.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        pass\n",
    "\n",
    "    for query in topic_queries:\n",
    "        offset = 0\n",
    "        while len(all_papers) < max_papers:\n",
    "            data, _ = query_api(search_url, query, offset=offset, limit=limit)\n",
    "            results = data.get(\"results\", [])\n",
    "            if not results:\n",
    "                break\n",
    "\n",
    "            for paper in results:\n",
    "                if paper.get(\"fullText\") and paper.get(\"id\") not in seen_ids:\n",
    "                    seen_ids.add(paper[\"id\"])\n",
    "                    all_papers.append(paper)\n",
    "\n",
    "            offset += limit\n",
    "            time.sleep(2)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        if len(all_papers) >= max_papers:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n‚úÖ Total collected papers with full text: {len(all_papers)}\")\n",
    "\n",
    "    with open(\"core_fulltext_dataset_filtered.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for paper in all_papers:\n",
    "            full_text = paper.get(\"fullText\", \"\")\n",
    "            sections = extract_sections(full_text)\n",
    "            abstract_or_intro = get_abstract_or_intro(paper)\n",
    "            if not abstract_or_intro:\n",
    "                continue\n",
    "            if all(sections[k].startswith(\"No \") for k in [\"conclusions\", \"future_work\", \"limitations\"]):\n",
    "                continue\n",
    "\n",
    "            record = {\n",
    "                \"abstract\": abstract_or_intro,\n",
    "                \"conclusions\": sections[\"conclusions\"],\n",
    "                \"limitations\": sections[\"limitations\"],\n",
    "                \"future_work\": sections[\"future_work\"]\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"üìÇ Saved to core_fulltext_dataset_filtered.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d16f02",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è 4. Run the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a733fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
