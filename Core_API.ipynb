{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7e53c3",
   "metadata": {},
   "source": [
    "# üì¶ 0. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7444c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/murat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure punkt tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Ensure punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "# Your API key\n",
    "apikey = 'bgJyXuHdGkrBKt4VsCvR0LeiwE8x39WZ'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1ea2",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010f4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def safe_get(text):\n",
    "    return text.strip() if isinstance(text, str) and text.strip() else None\n",
    "\n",
    "def get_abstract_or_intro(paper):\n",
    "    abstract = safe_get(paper.get(\"abstract\"))\n",
    "    if abstract:\n",
    "        return abstract\n",
    "\n",
    "    full_text = paper.get(\"fullText\", \"\")\n",
    "    full_text = clean_text(full_text)\n",
    "\n",
    "    intro_match = re.search(\n",
    "        r'(?:^|\\n)(?:\\d?\\s*INTRODUCTION|BACKGROUND)(?:[:\\.\\n\\s]+)(.*?)(?=\\n[A-Z ]{3,}|[\\n]{2,})',\n",
    "        full_text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if intro_match:\n",
    "        intro = intro_match.group(1).strip()\n",
    "        if len(intro) > 100:\n",
    "            return \"Introduction: \" + intro\n",
    "\n",
    "    return None\n",
    "\n",
    "def query_api(search_url, query, offset=0, limit=20):\n",
    "    headers = {\"Authorization\": \"Bearer \" + apikey}\n",
    "    url = f\"{search_url}?q={query}&limit={limit}&offset={offset}\"\n",
    "\n",
    "    for _ in range(3):  # Retry logic\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                print(f\"Query: {query} | Offset: {offset} | Results: {len(data.get('results', []))}\")\n",
    "                return data, response.elapsed.total_seconds()\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è JSON decode error at offset {offset} for query '{query}'\")\n",
    "                return {\"results\": []}, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error {response.status_code}, retrying...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è Failed after retries for query='{query}', offset={offset}\")\n",
    "    return {\"results\": []}, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9301b",
   "metadata": {},
   "source": [
    "# üßê 2. Section Extraction Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979a4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sections(full_text):\n",
    "    if not full_text:\n",
    "        return {\n",
    "            \"conclusions\": \"Full text not available\",\n",
    "            \"limitations\": \"Full text not available\",\n",
    "            \"future_work\": \"Full text not available\"\n",
    "        }\n",
    "\n",
    "    full_text = clean_text(full_text)\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n|\\r\\n\\s*\\r\\n', full_text) if p.strip()]\n",
    "    sections = []\n",
    "    current_section = {\"heading\": \"\", \"content\": \"\"}\n",
    "\n",
    "    for p in paragraphs:\n",
    "        if len(p) < 100 and (p.isupper() or re.match(r'^\\d+[\\.\\s]+\\w+|^[IVX]+[\\.\\s]+\\w+', p)):\n",
    "            if current_section[\"content\"]:\n",
    "                sections.append(current_section)\n",
    "            current_section = {\"heading\": p, \"content\": \"\"}\n",
    "        else:\n",
    "            current_section[\"content\"] += (\" \" + p) if current_section[\"content\"] else p\n",
    "    if current_section[\"content\"]:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    patterns = {\n",
    "        \"limitations\": [\n",
    "            r'\\b(?:limitation|shortcoming|drawback|weakness|constraint)s?\\b',\n",
    "            r'\\bcurrent\\s+(?:limitation|constraint|shortcoming)s?\\b',\n",
    "            r'\\blimiting\\s+factor[s]?\\b',\n",
    "            r'\\bsources?\\s+of\\s+error\\b'\n",
    "        ],\n",
    "        \"future_work\": [\n",
    "            r'\\bfuture\\s+(?:work|research|direction|study|investigation)\\b',\n",
    "            r'\\bfurther\\s+(?:work|research|study|development)\\b',\n",
    "            r'\\bopen\\s+(?:question|issue|challenge|problem|area)s?\\b',\n",
    "            r'\\bwe\\s+plan\\s+to\\b'\n",
    "        ],\n",
    "        \"conclusions\": [\n",
    "            r'\\bconclusion[s]?\\b',\n",
    "            r'\\bconcluding\\s+remarks\\b',\n",
    "            r'\\bin\\s+conclusion\\b',\n",
    "            r'\\bthis\\s+study\\s+(?:shows|demonstrates|confirms|indicates)\\b'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    output = {\"limitations\": \"\", \"future_work\": \"\", \"conclusions\": \"\"}\n",
    "\n",
    "    for section in sections:\n",
    "        heading_lower = section[\"heading\"].lower()\n",
    "        for key, regex_list in patterns.items():\n",
    "            if any(re.search(p, heading_lower) for p in regex_list):\n",
    "                output[key] += f\"Section: {section['heading']}\\n{section['content']}\\n\\n\"\n",
    "\n",
    "    for key, regex_list in patterns.items():\n",
    "        if not output[key]:\n",
    "            matched_sentences = []\n",
    "            for section in sections:\n",
    "                sentences = sent_tokenize(section[\"content\"])\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    if any(re.search(p, sentence.lower()) for p in regex_list):\n",
    "                        context = sentences[max(0, i-1):min(len(sentences), i+2)]\n",
    "                        matched_sentences.append(\" \".join(context))\n",
    "            if matched_sentences:\n",
    "                output[key] = \"Auto-extracted mentions:\\n\" + \"\\n\".join(matched_sentences)\n",
    "\n",
    "    for key in output:\n",
    "        if not output[key]:\n",
    "            output[key] = f\"No {key.replace('_', ' ')} content found\"\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690bfea",
   "metadata": {},
   "source": [
    "# üöÄ 3. Main Loop for Querying and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Globals\n",
    "all_papers = []\n",
    "filtered_papers = []\n",
    "future_work_analysis = []\n",
    "\n",
    "# üöÄ 1. Collect Papers\n",
    "def collect_papers():\n",
    "    global all_papers\n",
    "    search_url = \"https://api.core.ac.uk/v3/search/works\"\n",
    "    topic_queries = [\n",
    "        \"artificial intelligence\", \"machine learning\", \"deep learning\", \"data science\", \"AI applications\",\n",
    "        \"natural language processing\", \"language models\", \"NLP\", \"text mining\", \"information extraction\",\n",
    "        \"computer vision\", \"image recognition\", \"object detection\", \"vision transformers\",\n",
    "        \"biomedical informatics\", \"health informatics\", \"clinical AI\", \"medical imaging\", \"EHR\", \"genomics\",\n",
    "        \"AI ethics\", \"explainable AI\", \"fairness in machine learning\", \"AI in education\", \"social computing\",\n",
    "        \"support vector machines\", \"random forests\", \"decision trees\", \"unsupervised learning\", \"feature selection\",\n",
    "        \"AI systems\", \"distributed learning\", \"edge AI\", \"federated learning\", \"hardware-aware ML\"\n",
    "    ]\n",
    "    \n",
    "    max_papers = 2000\n",
    "    limit = 20\n",
    "    seen_ids = set()\n",
    "\n",
    "    for query in topic_queries:\n",
    "        offset = 0\n",
    "        while len(all_papers) < max_papers:\n",
    "            data, _ = query_api(search_url, query, offset=offset, limit=limit)\n",
    "            results = data.get(\"results\", [])\n",
    "            if not results:\n",
    "                break\n",
    "\n",
    "            for paper in results:\n",
    "                if paper.get(\"fullText\") and paper.get(\"id\") not in seen_ids:\n",
    "                    seen_ids.add(paper[\"id\"])\n",
    "                    all_papers.append(paper)\n",
    "\n",
    "            offset += limit\n",
    "            time.sleep(2)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        if len(all_papers) >= max_papers:\n",
    "            break\n",
    "\n",
    "    # Save raw papers immediately\n",
    "    with open(\"core_raw_fulltext_collected.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in all_papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Collected and saved {len(all_papers)} raw papers.\")\n",
    "\n",
    "# üß† 2. Filter and Structure Papers\n",
    "def filter_papers():\n",
    "    global filtered_papers\n",
    "    for paper in all_papers:\n",
    "        full_text = paper.get(\"fullText\", \"\")\n",
    "        sections = extract_sections(full_text)\n",
    "        abstract_or_intro = get_abstract_or_intro(paper)\n",
    "        if not abstract_or_intro:\n",
    "            continue\n",
    "        if all(sections[k].startswith(\"No \") for k in [\"conclusions\", \"future_work\", \"limitations\"]):\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"abstract\": abstract_or_intro,\n",
    "            \"conclusions\": sections[\"conclusions\"],\n",
    "            \"limitations\": sections[\"limitations\"],\n",
    "            \"future_work\": sections[\"future_work\"]\n",
    "        }\n",
    "        filtered_papers.append(record)\n",
    "\n",
    "    # Save filtered papers\n",
    "    with open(\"core_fulltext_dataset_filtered.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in filtered_papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Filtered and saved {len(filtered_papers)} papers with required sections.\")\n",
    "\n",
    "# üß™ 3. Analyze Future Work Sections\n",
    "def analyze_future_work():\n",
    "    global future_work_analysis\n",
    "    for paper in filtered_papers:\n",
    "        future_work_text = paper.get(\"future_work\", \"\")\n",
    "        if future_work_text and \"No future work\" not in future_work_text:\n",
    "            analysis = {\n",
    "                \"abstract_snippet\": paper[\"abstract\"][:300] + \"...\",\n",
    "                \"future_work_summary\": future_work_text\n",
    "            }\n",
    "            future_work_analysis.append(analysis)\n",
    "\n",
    "    # Save future work analysis\n",
    "    with open(\"future_work_analysis.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in future_work_analysis:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Analyzed and saved {len(future_work_analysis)} future work entries.\")\n",
    "\n",
    "# ‚ñ∂Ô∏è Full Pipeline\n",
    "def run_full_pipeline():\n",
    "    collect_papers()\n",
    "    filter_papers()\n",
    "    analyze_future_work()\n",
    "    print(\"üèÅ Full pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d16f02",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è 4. Run the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a733fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_full_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# filter_papers()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mrun_full_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_full_pipeline\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mcollect_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     filter_papers()\n\u001b[32m    102\u001b[39m     analyze_future_work()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mcollect_papers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m offset = \u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_papers) < max_papers:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     data, _ = \u001b[43mquery_api\u001b[49m(search_url, query, offset=offset, limit=limit)\n\u001b[32m     28\u001b[39m     results = data.get(\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "\u001b[31mNameError\u001b[39m: name 'query_api' is not defined"
     ]
    }
   ],
   "source": [
    "run_full_pipeline()\n",
    "# filter_papers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff264a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
